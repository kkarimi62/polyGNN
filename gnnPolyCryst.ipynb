{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69e71df7",
   "metadata": {},
   "source": [
    "# Hardness properties of a polycrystal\n",
    "This notebook and the accompanying code demonstrates how to use the Graph Nets library to learn to predict the hardness map of a polycrystal ([Karimi et al. Scripta Materialia, 234:115559, 2023](https://doi.org/10.1016/j.scriptamat.2023.115559)).\n",
    "\n",
    "The network is trained to predict the load-depth curves of nanoindented grains in steel. \n",
    "\n",
    "Upon training, the network's prediction ability is illustrated by comparing its output to the true hardness of the material."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f985dd4",
   "metadata": {},
   "source": [
    "<img src=\"workFlow.jpg\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bbd906",
   "metadata": {},
   "source": [
    "# Copyright Notice\n",
    "\n",
    "Â© 2023 **Kamran Karimi**. All rights reserved.\n",
    "\n",
    "This notebook is licensed under the [Creative Commons Attribution 4.0 International License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "---\n",
    "\n",
    "## Contact Information\n",
    "\n",
    "For any inquiries or further information, please contact:\n",
    "\n",
    "- **Email:** [kamran.karimi@ncbj.gov.pl](mailto:kamran.karimni@ncbj.gov.pl)\n",
    "\n",
    "Feel free to reach out with any questions or feedback."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d6340b",
   "metadata": {},
   "source": [
    "# Install dependencies locally\n",
    "\n",
    "If you are running this notebook locally (i.e., not through Colaboratory), you will also need to install a few more dependencies. Run the following on the command line to install the graph networks library, as well as a few other dependencies:\n",
    "\n",
    "```\n",
    "pip install graph_nets matplotlib scipy \"tensorflow>=1.15,<2\" \"dm-sonnet<2\" \"tensorflow_probability<0.9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "326bab34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping installation of Graph Nets library\n"
     ]
    }
   ],
   "source": [
    "install_graph_nets_library = \"No\"  #param [\"Yes\", \"No\"]\n",
    "\n",
    "if install_graph_nets_library.lower() == \"yes\":\n",
    "  print(\"Installing Graph Nets library and dependencies:\")\n",
    "  print(\"Output message from command:\\n\")\n",
    "  !pip install graph_nets \"dm-sonnet<2\" \"tensorflow_probability<0.9\"\n",
    "else:\n",
    "  print(\"Skipping installation of Graph Nets library\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3adda139",
   "metadata": {},
   "source": [
    "# Load configuration file & libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e1425d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#--- for parsing configuration files\n",
    "import configparser\n",
    "\n",
    "#--- plotting\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MultipleLocator, AutoMinorLocator\n",
    "matplotlib.rcParams['text.usetex'] = True \n",
    "\n",
    "#--- interpolation  \n",
    "from scipy.interpolate import Rbf\n",
    "from scipy.stats import zscore\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "import sonnet as snt\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "#--- graph nets\n",
    "from graph_nets import graphs\n",
    "from graph_nets import modules as gn_modules\n",
    "from graph_nets import utils_tf\n",
    "from graph_nets import utils_np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a9532d",
   "metadata": {},
   "source": [
    "Here is the implementation of the graph net model in tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "2b516552",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "NUM_LAYERS       = 2  # Hard-code number of layers in the edge/node/global models.\n",
    "LATENT_SIZE      = 16  # Hard-code latent layer sizes for demos.\n",
    "\n",
    "GLOBAL_MODEL_FN=lambda: lambda x: x\n",
    "  \n",
    "NODE_NUM_LAYERS  = 2  # Hard-code number of layers in the edge/node/global models.\n",
    "NODE_LATENT_SIZE = 8  # Hard-code latent layer sizes for demos.\n",
    "EDGE_NUM_LAYERS  = 2  # Hard-code number of layers in the edge/node/global models.\n",
    "EDGE_LATENT_SIZE = 8  # Hard-code latent layer sizes for demos.\n",
    "GLOB_NUM_LAYERS  = 2  # Hard-code number of layers in the edge/node/global models.\n",
    "GLOB_LATENT_SIZE = 32  # Hard-code latent layer sizes for demos.\n",
    "\n",
    "ACT_FINAL        = False\n",
    "\n",
    "def make_mlp_model():\n",
    "  \"\"\"Instantiates a new MLP, followed by LayerNorm.\n",
    "  The parameters of each new MLP are not shared with others generated by\n",
    "  this function.\n",
    "  Returns:\n",
    "    A Sonnet module \n",
    "  \"\"\"\n",
    "  return snt.Sequential([\n",
    "      snt.nets.MLP([LATENT_SIZE] * NUM_LAYERS, activate_final=ACT_FINAL, activation=tf.keras.activations.tanh), #False, activation=tf.keras.activations.tanh),\n",
    "      snt.LayerNorm()\n",
    "  ])\n",
    "\n",
    "\n",
    "def make_mlp_model_edge():\n",
    "  \"\"\"Instantiates a new MLP, followed by LayerNorm.\n",
    "  The parameters of each new MLP are not shared with others generated by\n",
    "  this function.\n",
    "  Returns:\n",
    "    A Sonnet module \n",
    "  \"\"\"\n",
    "  return snt.Sequential([\n",
    "      snt.nets.MLP([EDGE_LATENT_SIZE] * EDGE_NUM_LAYERS, activate_final=ACT_FINAL, activation=tf.keras.activations.tanh), #False, activation=tf.keras.activations.tanh),\n",
    "      snt.LayerNorm()\n",
    "  ])\n",
    "\n",
    "def make_mlp_model_node():\n",
    "  \"\"\"Instantiates a new MLP, followed by LayerNorm.\n",
    "  The parameters of each new MLP are not shared with others generated by\n",
    "  this function.\n",
    "  Returns:\n",
    "    A Sonnet module \n",
    "  \"\"\"\n",
    "  return snt.Sequential([\n",
    "      snt.nets.MLP([NODE_LATENT_SIZE] * NODE_NUM_LAYERS, activate_final=ACT_FINAL, activation=tf.keras.activations.tanh), #False, activation=tf.keras.activations.tanh),\n",
    "      snt.LayerNorm()\n",
    "  ])\n",
    "\n",
    "def make_mlp_model_global():\n",
    "  \"\"\"Instantiates a new MLP, followed by LayerNorm.\n",
    "  The parameters of each new MLP are not shared with others generated by\n",
    "  this function.\n",
    "  Returns:\n",
    "    A Sonnet module \n",
    "  \"\"\"\n",
    "  return snt.Sequential([\n",
    "      snt.nets.MLP([GLOB_LATENT_SIZE] * GLOB_NUM_LAYERS, activate_final=ACT_FINAL, activation=tf.keras.activations.tanh), #False, activation=tf.keras.activations.tanh),\n",
    "      snt.LayerNorm()\n",
    "  ])\n",
    "\n",
    "class MLPGraphIndependent(snt.AbstractModule):\n",
    "  \"\"\"GraphIndependent with MLP edge, node, and global models.\"\"\"\n",
    "\n",
    "  def __init__(self, name=\"MLPGraphIndependent\"):\n",
    "    super(MLPGraphIndependent, self).__init__(name=name)\n",
    "    with self._enter_variable_scope():\n",
    "      self._network = modules.GraphIndependent(\n",
    "          edge_model_fn=make_mlp_model_edge,\n",
    "          node_model_fn=make_mlp_model_node,\n",
    "          global_model_fn=make_mlp_model_global)\n",
    "\n",
    "  def _build(self, inputs):\n",
    "    return self._network(inputs)\n",
    "\n",
    "\n",
    "class MLPGraphNetwork(snt.AbstractModule):\n",
    "  \"\"\"GraphNetwork with MLP edge, node, and global models.\"\"\"\n",
    "\n",
    "  def __init__(self, name=\"MLPGraphNetwork\"):\n",
    "    super(MLPGraphNetwork, self).__init__(name=name)\n",
    "    with self._enter_variable_scope():\n",
    "      self._network = modules.GraphNetwork(edge_model_fn=make_mlp_model_edge, node_model_fn=make_mlp_model_node,\n",
    "                                           global_model_fn=make_mlp_model_global)\n",
    "\n",
    "  def _build(self, inputs):\n",
    "    return self._network(inputs)\n",
    "\n",
    "\n",
    "class EncodeProcessDecode(snt.AbstractModule):\n",
    "  \"\"\"Full encode-process-decode model.\n",
    "  The model we explore includes three components:\n",
    "  - An \"Encoder\" graph net, which independently encodes the edge, node, and\n",
    "    global attributes (does not compute relations etc.).\n",
    "  - A \"Core\" graph net, which performs N rounds of processing (message-passing)\n",
    "    steps. The input to the Core is the concatenation of the Encoder's output\n",
    "    and the previous output of the Core (labeled \"Hidden(t)\" below, where \"t\" is\n",
    "    the processing step).\n",
    "  - A \"Decoder\" graph net, which independently decodes the edge, node, and\n",
    "    global attributes (does not compute relations etc.), on each message-passing\n",
    "    step.\n",
    "                      Hidden(t)   Hidden(t+1)\n",
    "                         |            ^\n",
    "            *---------*  |  *------*  |  *---------*\n",
    "            |         |  |  |      |  |  |         |\n",
    "  Input --->| Encoder |  *->| Core |--*->| Decoder |---> Output(t)\n",
    "            |         |---->|      |     |         |\n",
    "            *---------*     *------*     *---------*\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               edge_output_size=None,\n",
    "               node_output_size=None,\n",
    "               global_output_size=None,\n",
    "               name=\"EncodeProcessDecode\"):\n",
    "    super(EncodeProcessDecode, self).__init__(name=name)\n",
    "    self._encoder = MLPGraphIndependent()\n",
    "    self._core = MLPGraphNetwork()\n",
    "    self._decoder = MLPGraphIndependent()\n",
    "    # Transforms the outputs into the appropriate shapes.\n",
    "    if edge_output_size is None:\n",
    "      edge_fn = None\n",
    "    else:\n",
    "      edge_fn = lambda: snt.Linear(edge_output_size, name=\"edge_output\")\n",
    "    if node_output_size is None:\n",
    "      node_fn = None\n",
    "    else:\n",
    "      node_fn = lambda: snt.Linear(node_output_size, name=\"node_output\")\n",
    "    if global_output_size is None:\n",
    "      global_fn = None\n",
    "    else:\n",
    "      global_fn = lambda: snt.Linear(global_output_size, name=\"global_output\")\n",
    "    with self._enter_variable_scope():\n",
    "      self._output_transform = modules.GraphIndependent(edge_fn, node_fn,\n",
    "                                                        global_fn)\n",
    "\n",
    "  def _build(self, input_op, num_processing_steps):\n",
    "    latent = self._encoder(input_op)\n",
    "    latent0 = latent\n",
    "\n",
    "    latent_ops = [latent]\n",
    "    output_ops = []\n",
    "    for _ in range(num_processing_steps):\n",
    "      core_input = utils_tf.concat([latent0, latent], axis=1)\n",
    "      latent = self._core(core_input)\n",
    "      latent_ops.append(latent)\n",
    "      decoded_op = self._decoder(latent)\n",
    "      output_ops.append(self._output_transform(decoded_op))\n",
    "    return output_ops, latent_ops\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb6bdaf",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "Let's define some utility functions for plotting and data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0aa4af70",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Legends:\n",
    "    def __init__(self\n",
    "                ):\n",
    "        pass\n",
    "    def Set(self,fontsize=20,\n",
    "                 labelspacing=0,\n",
    "                 **kwargs\n",
    "           ):\n",
    "        self.attrs = {'frameon':False,'fontsize':fontsize,\n",
    "                   'labelspacing':labelspacing,\n",
    "                      'handletextpad':.2,\n",
    "                   'handlelength':1,\n",
    "                    **kwargs,\n",
    "                     }\n",
    "    def Get(self):\n",
    "        return self.attrs\n",
    "\n",
    "def PutMinorTicks(ax, LOGY=None,LOGX=None, nevery_x=1,nevery_y=1):\n",
    "    ax.xaxis.set_minor_locator(AutoMinorLocator(2))\n",
    "    ax.yaxis.set_minor_locator(AutoMinorLocator(2))\n",
    "    if LOGY:\n",
    "        #--- add major yticks\n",
    "        ymin   = np.ceil(np.log10(ax.axis()[2]))\n",
    "        ymax   = np.floor(np.log10(ax.axis()[3]))\n",
    "        nbin   = ymax - ymin\n",
    "        ax.set_yticks(10**np.arange(ymin,ymax+nevery_y,nevery_y))\n",
    "        #--- put minor bins y\n",
    "        locmin = matplotlib.ticker.LogLocator(base=10.0,subs=(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1),numticks=12)\n",
    "        ax.yaxis.set_minor_locator(locmin)\n",
    "        ax.yaxis.set_minor_formatter(matplotlib.ticker.NullFormatter())\n",
    "    if LOGX:\n",
    "        #--- add major yticks\n",
    "        ymin   = np.ceil(np.log10(ax.axis()[0]))\n",
    "        ymax   = np.floor(np.log10(ax.axis()[1]))\n",
    "        nbin   = ymax - ymin\n",
    "        ax.set_xticks(10**np.arange(ymin,ymax+nevery_x,nevery_x))\n",
    "        #--- put minor bins y\n",
    "        locmin = matplotlib.ticker.LogLocator(base=10.0,subs=(0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1),numticks=12)\n",
    "        ax.xaxis.set_minor_locator(locmin)\n",
    "        ax.xaxis.set_minor_formatter(matplotlib.ticker.NullFormatter())\n",
    "\n",
    "        \n",
    "\n",
    "def PltErr( xdata, ydata, \n",
    "            yerr = None,\n",
    "            xstr = '',\n",
    "            ystr = '',\n",
    "            Plot = True,\n",
    "            **kwargs\n",
    "            ):\n",
    "    fontsize = kwargs['fontsize'] if 'fontsize' in kwargs else 20\n",
    "    if not 'ax' in kwargs:\n",
    "        fig  = plt.figure( figsize = (4,4) if 'figsize' not in kwargs else kwargs['figsize'] )\n",
    "        ax   = fig.add_subplot(111)\n",
    "        ax.markerss=['o','s','D','^','<','>','v']\n",
    "    else:\n",
    "        ax   = kwargs['ax']\n",
    "\n",
    "        if 'twinx' in kwargs and kwargs['twinx']:\n",
    "            ax = kwargs['ax'].twinx()\n",
    "    #--- setting   \n",
    "    ax.set_xlabel(xstr,fontsize=fontsize)\n",
    "    ax.set_ylabel(ystr,fontsize=fontsize)\n",
    "    ax.tick_params(labelsize=fontsize,which='both',axis='both', top=True, right=True)\n",
    "    #\n",
    "    xerr = kwargs['xerr'] if 'xerr' in kwargs else None \n",
    "#\n",
    "    if 'attrs' in kwargs:\n",
    "        ax.errorbar( xdata, ydata,yerr = yerr, xerr = xerr, **kwargs['attrs'])\n",
    "        if 'fill_between' in kwargs and kwargs['fill_between']:\n",
    "            ax.fill_between(xdata, ydata-yerr, ydata+yerr)\n",
    "    else:\n",
    "        ax.errorbar( xdata, ydata,yerr = yerr, xerr = xerr,\n",
    "                    fmt=kwargs['fmt'] if 'fmt' in kwargs else '-o',\n",
    "                    label=kwargs['label'] if 'label' in kwargs else '',\n",
    "                    markevery=kwargs['markevery'] if 'markevery' in kwargs else 1,\n",
    "                    markersize=kwargs['markersize'] if 'markersize' in kwargs else 10,\n",
    "                    marker=kwargs['marker'] if 'marker' in kwargs else 'o', #ax.markerss[(ax.count)%7],\n",
    "                   )\n",
    "\n",
    "    #--- plot\n",
    "    if 'ylim' in kwargs:\n",
    "        ylim = kwargs['ylim'] \n",
    "        ax.set_ylim(ylim)\n",
    "    if 'xlim' in kwargs:\n",
    "        xlim = kwargs['xlim'] \n",
    "        ax.set_xlim(xlim)\n",
    "    #\n",
    "    if 'xscale' in kwargs: \n",
    "        ax.set_xscale(kwargs['xscale'])\n",
    "    if 'yscale' in kwargs: \n",
    "        ax.set_yscale(kwargs['yscale'])\n",
    "    #\n",
    "    if 'xticks' in kwargs:\n",
    "        ax.set_xticks(list(map(float,kwargs['xticks'][1])))\n",
    "#        ax.set_xticklabels(list(map(lambda x:'$%s$'%x,kwargs['xticks'][0])))\n",
    "        ax.set_xticklabels(kwargs['xticks'][0])\n",
    "    #\n",
    "    if 'yticks' in kwargs:\n",
    "        ax.set_yticks(list(map(float,kwargs['yticks'][1])))\n",
    "        ax.set_yticklabels(list(map(lambda x:'$%s$'%x,kwargs['yticks'][0])))\n",
    "        \n",
    "    #\n",
    "    LOGY      = True if ('yscale' in kwargs and kwargs['yscale'] == 'log') else False\n",
    "    LOGX      = True if ('xscale' in kwargs and kwargs['xscale'] == 'log') else False\n",
    "    ndecade_x = kwargs['ndecade_x'] if 'ndecade_x' in kwargs else 1\n",
    "    ndecade_y = kwargs['ndecade_y'] if 'ndecade_y' in kwargs else 1\n",
    "    PutMinorTicks(ax, LOGX=LOGX,LOGY=LOGY,nevery_x=ndecade_x,nevery_y=ndecade_y)\n",
    "    #\n",
    "    if 'DrawFrame' in kwargs: \n",
    "        DrawFrame(ax, *kwargs['DrawFrame'],LOG_Y=LOGY,LOG_X=LOGX)\n",
    "    #\n",
    "    if 'legend' in kwargs:\n",
    "        plt.legend(**kwargs['legend'])\n",
    "\t#\n",
    "    if 'halfopen' in kwargs and kwargs['halfopen']:\n",
    "        ax.spines['right'].set_visible(False) #--- half open\n",
    "        ax.spines['top'].set_visible(False)\n",
    "        ax.xaxis.set_ticks_position('bottom')\n",
    "        ax.yaxis.set_ticks_position('left')\n",
    "\n",
    "    if 'set_title' in kwargs: #Plot:\n",
    "        ax.set_title(kwargs['set_title'],fontsize=fontsize )\n",
    "    if 'title' in kwargs: #Plot:\n",
    "        plt.savefig(kwargs['title'],dpi=300 if not 'dpi' in kwargs else kwargs['dpi'],bbox_inches='tight', \n",
    "                    pad_inches=0.0)\n",
    "    if Plot:\n",
    "        plt.show()\n",
    "    return ax\n",
    "\n",
    "\n",
    "def FilterDataFrame(test_data_grains,key='id',val=[1,2,3],out='index'):\n",
    "    tmp = pd.DataFrame(np.c_[np.arange(test_data_grains.shape[0]),test_data_grains],columns=[out]+list(test_data_grains.keys()))\n",
    "    return np.c_[tmp.set_index(key,drop=True,append=False).loc[val][out]].flatten().astype(int)\n",
    "\n",
    "    \n",
    "def base_graph(df_attributes, \n",
    "                df_pairwise_attributes, \n",
    "                disps,\n",
    "                forces,\n",
    "                attributes\n",
    "              ):\n",
    "    \"\"\"\n",
    "    This function loads the data and forms a graph structure. This should be implemented as it is dataset-dependent.\n",
    "    Output should be:\n",
    "        a dict with  globals (dummy), nodes (nodal data in numpy), edges (edge data), receivers (indices of receiving node in int), senders (int)\n",
    "        train_mask   array of size (n_nodes) with bool or 0/1 indicating training nodes\n",
    "        val_mask     same for validation nodes \n",
    "        test_mask    same for testing nodes\n",
    "        target       here the array containing the nodal target data\n",
    "        weight       if needed, a weight parameter given for the (training) nodes\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    positions = np.c_[df_attributes.apply(zscore)[attributes],zscore(forces)].tolist()\n",
    "    edges     = np.c_[df_pairwise_attributes[['misOrientationAngle(deg)','boundaryLength(micron)']]].tolist()\n",
    "    \n",
    "    grain_i_indices = FilterDataFrame(df_attributes,key='#grainID',val=df_pairwise_attributes['#grain_i_ID'],out='index')\n",
    "    grain_j_indices = FilterDataFrame(df_attributes,key='#grainID',val=df_pairwise_attributes['grain_j_ID'],out='index')\n",
    "\n",
    "    receivers = list(grain_j_indices)\n",
    "    senders   = list(grain_i_indices) \n",
    "    \n",
    "    target = list(map(lambda x:list(x),disps))\n",
    "    weight = list(np.ones(df_attributes.shape[0]))\n",
    "\n",
    "    return {\"globals\": [0.0],  \"nodes\": positions, \"edges\": edges,  \n",
    "            \"receivers\": receivers, \"senders\": senders  },target, weight \n",
    "\n",
    "def create_loss_ops(target_op, output_op, mask, weight=None):\n",
    "    \"\"\"Create supervised loss operations from targets and outputs.\n",
    "\n",
    "    Args:\n",
    "      target_op: The target tf.Tensor.\n",
    "      output_ops: The output graph from the model.\n",
    "\n",
    "    Returns:\n",
    "      loss values (tf.Tensor)\n",
    "    \"\"\"\n",
    "    if weight is None:\n",
    "        loss_op = tf.reduce_mean(  (  tf.boolean_mask(output_op.nodes, mask) - tf.boolean_mask(target_op, mask))**2)\n",
    "    else:\n",
    "        loss_op = tf.reduce_mean( tf.boolean_mask(weight, mask)* (  tf.boolean_mask(output_op.nodes, mask) - tf.boolean_mask(target_op, mask))**2)\n",
    "\n",
    "    return loss_op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90dea8c",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "Our dataset consists of a collection of grain-level load-depth curves corresponding to\n",
    "nano-indentation tests performed on a polycrystalline steel. \n",
    "The dataset is organized within the `data/loadCurves` directory with relevant data files labeled as `GrainID_${grain_id}_LoadID${load_id}_IndentLabel_${indent_label}.txt`. \n",
    "Here `grain_id` denotes grain id and `load_id=0...10` corresponds to the maximum indentation load used in each experimental set with `fmax = 0.25, 0.5, 0.75, 1, 1.25, 1.5, 1.75, 2, 2.5, 3, 3.5, 4, 4.5, 5, 7, 10 mN`.\n",
    "The variable `indent_label=1...15` is associated with the same fmax but at different indentation points as labeled in the EBSD map below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0fb62a",
   "metadata": {},
   "source": [
    "<img src=\"ebsd.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc932c1e",
   "metadata": {},
   "source": [
    "The raw data set (including the load-depth curves and EBSD file) can be accessed through the following link: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7975920.svg)](https://doi.org/10.5281/zenodo.7975920)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d0dc4d",
   "metadata": {},
   "source": [
    "These files include two columns representing the indentation depth (nm) and force (mN) measurements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "f412f7cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_loadCurves        = os.listdir('data/loadCurves')\n",
    "loadCurves             = {item:np.loadtxt( 'data/loadCurves/%s'%item ) for item in list_loadCurves}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "b6fdbace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='depth(nm)', ylabel='load(mN)'>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_depth = loadCurves['loadDepth_GrainID_211_LoadID4_IndentLabel_1.txt']\n",
    "PltErr(load_depth[:,0],load_depth[:,1],\n",
    "      xstr='depth(nm)',ystr='load(mN)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c69c06d",
   "metadata": {},
   "source": [
    "The grains' attributes are included in `data/attributes.txt` and `data/pairwise_attributes.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "da4c8d39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5364, 16)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_attributes          = pd.read_csv('data/attributes.txt',sep=' ')\n",
    "df_pairwise_attributes = pd.read_csv('data/pairwise_attributes.txt',sep=' ')\n",
    "\n",
    "df_attributes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6b557bba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#grainID</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>area</th>\n",
       "      <th>perimeter</th>\n",
       "      <th>subBoundaryLength</th>\n",
       "      <th>diameter</th>\n",
       "      <th>equivalentPerimeter</th>\n",
       "      <th>shapeFactor</th>\n",
       "      <th>isBoundary</th>\n",
       "      <th>hasHole</th>\n",
       "      <th>isInclusion</th>\n",
       "      <th>numNeighbors</th>\n",
       "      <th>phi1</th>\n",
       "      <th>Phi</th>\n",
       "      <th>phi2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-27.71273</td>\n",
       "      <td>0.261343</td>\n",
       "      <td>5.154668</td>\n",
       "      <td>9.546976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.467543</td>\n",
       "      <td>8.048321</td>\n",
       "      <td>1.186207</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>224.48090</td>\n",
       "      <td>82.65546</td>\n",
       "      <td>344.2817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-31.17682</td>\n",
       "      <td>0.261343</td>\n",
       "      <td>5.154668</td>\n",
       "      <td>9.546976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.467543</td>\n",
       "      <td>8.048321</td>\n",
       "      <td>1.186207</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10.05254</td>\n",
       "      <td>62.38479</td>\n",
       "      <td>165.6788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>-69.28182</td>\n",
       "      <td>0.261343</td>\n",
       "      <td>5.154668</td>\n",
       "      <td>9.546976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.467543</td>\n",
       "      <td>8.048321</td>\n",
       "      <td>1.186207</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>82.06073</td>\n",
       "      <td>65.44725</td>\n",
       "      <td>242.2282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-136.83160</td>\n",
       "      <td>-0.809521</td>\n",
       "      <td>4.041427</td>\n",
       "      <td>8.082879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.055041</td>\n",
       "      <td>7.126434</td>\n",
       "      <td>1.134211</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>307.84280</td>\n",
       "      <td>79.36325</td>\n",
       "      <td>150.2078</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-140.29570</td>\n",
       "      <td>-0.809521</td>\n",
       "      <td>4.041427</td>\n",
       "      <td>8.082879</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.055041</td>\n",
       "      <td>7.126434</td>\n",
       "      <td>1.134211</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>332.46850</td>\n",
       "      <td>150.01300</td>\n",
       "      <td>303.3176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #grainID          x         y      area  perimeter  subBoundaryLength  \\\n",
       "0         1  -27.71273  0.261343  5.154668   9.546976                0.0   \n",
       "1         2  -31.17682  0.261343  5.154668   9.546976                0.0   \n",
       "2         3  -69.28182  0.261343  5.154668   9.546976                0.0   \n",
       "3         4 -136.83160 -0.809521  4.041427   8.082879                0.0   \n",
       "4         5 -140.29570 -0.809521  4.041427   8.082879                0.0   \n",
       "\n",
       "   diameter  equivalentPerimeter  shapeFactor  isBoundary  hasHole  \\\n",
       "0  3.467543             8.048321     1.186207           1        0   \n",
       "1  3.467543             8.048321     1.186207           1        0   \n",
       "2  3.467543             8.048321     1.186207           1        0   \n",
       "3  3.055041             7.126434     1.134211           0        0   \n",
       "4  3.055041             7.126434     1.134211           0        0   \n",
       "\n",
       "   isInclusion  numNeighbors       phi1        Phi      phi2  \n",
       "0            0             3  224.48090   82.65546  344.2817  \n",
       "1            0             3   10.05254   62.38479  165.6788  \n",
       "2            0             3   82.06073   65.44725  242.2282  \n",
       "3            0             2  307.84280   79.36325  150.2078  \n",
       "4            0             2  332.46850  150.01300  303.3176  "
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_attributes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "125cee74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#grain_i_ID</th>\n",
       "      <th>grain_j_ID</th>\n",
       "      <th>misOrientationAngle(deg)</th>\n",
       "      <th>boundaryLength(micron)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>59.89713</td>\n",
       "      <td>0.154700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>41.99782</td>\n",
       "      <td>0.154700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>92</td>\n",
       "      <td>59.74895</td>\n",
       "      <td>5.773485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>59.94544</td>\n",
       "      <td>0.154700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>92</td>\n",
       "      <td>39.02650</td>\n",
       "      <td>5.773485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #grain_i_ID  grain_j_ID  misOrientationAngle(deg)  boundaryLength(micron)\n",
       "0            1           2                  59.89713                0.154700\n",
       "1            1          33                  41.99782                0.154700\n",
       "2            1          92                  59.74895                5.773485\n",
       "3            2          14                  59.94544                0.154700\n",
       "4            2          92                  39.02650                5.773485"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pairwise_attributes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4053630",
   "metadata": {},
   "source": [
    "## Preprocess data\n",
    "Here we discretize the load and depth vectors using the radial basis function implemented in python.\n",
    "Let's implement `class PreprocessLoadData` for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "70d4c7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessLoadData:\n",
    "    '''\n",
    "    Preprocess the load-depth data\n",
    "    '''\n",
    "    ld_curve = {}\n",
    "    load     = {}\n",
    "    \n",
    "    def __init__(self,data,verbose=False):\n",
    "        self.data    = data\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def is_included(self,xlo,xhi):\n",
    "        '''\n",
    "        return bool: timeseries includes the range (xlo,xhi) \n",
    "        '''\n",
    "        return self.data[:,0].min() <= xlo and self.data[:,0].max() >=  xhi   \n",
    "        \n",
    "    def Interp(self,nbins):\n",
    "        '''\n",
    "        interpolate on the structured grid bins\n",
    "        '''\n",
    "        return self.Rbf(self.data[:,0],nbins), self.Rbf(self.data[:,1],nbins)\n",
    "        \n",
    "    def Rbf(self,data,nbins):\n",
    "        x    = np.linspace(0, 1, data.shape[0])\n",
    "        d    = data \n",
    "        rbfi = Rbf(x, d)\n",
    "        xi   = np.linspace(0, 1, nbins)\n",
    "        return rbfi(xi)\n",
    "            \n",
    "    def GetGrainIndex(self, title, df_attributes ):\n",
    "        '''\n",
    "        return grain index and id \n",
    "        '''\n",
    "        GetGrainID = lambda x:x[x.find('ID_')+3:x.find('_LoadID')]\n",
    "        GrainID    = GetGrainID(title)\n",
    "        filtr      = df_attributes['#grainID']==float(GrainID)\n",
    "        assert np.any(filtr), 'grainID %s not exist!'%GrainID\n",
    "        return df_attributes[filtr].index[0]\n",
    "\n",
    "    def Scale(self):\n",
    "        '''\n",
    "        return scaled data \n",
    "        '''\n",
    "        self.data[:,0] /= np.max(self.data[:,1]) #--- scale by fmax\n",
    "        self.data[:,1] /= np.max(self.data[:,1])\n",
    "        \n",
    "    @staticmethod\n",
    "    def ReplaceNanByMean(forces):\n",
    "        mean_force_array = np.mean(forces[~np.any(np.isnan(forces),axis=1)],axis=0) #--- replace missing data by the average\n",
    "        isnan            = np.any(np.isnan(forces),axis=1)\n",
    "        \n",
    "        for grain_indx in range(len(forces)):\n",
    "            if isnan[grain_indx]:\n",
    "                forces[grain_indx] = np.copy(mean_force_array)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def Append(GrainIndex,disp,load):\n",
    "        '''\n",
    "        append stress timeseries\n",
    "        '''\n",
    "        PreprocessLoadData.ld_curve.setdefault(GrainIndex,[]).append(disp.copy()) \n",
    "        PreprocessLoadData.load.setdefault(GrainIndex,[]).append(load.copy())\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def BuildFeature(sdict,ngrains_total,n_ld):\n",
    "    #--- build feature matrix\n",
    "        keys = list(sdict.keys()) #--- indented grains\n",
    "        mat  = np.c_[list(map(lambda x:np.mean(np.c_[sdict[x]],axis=0),keys))] #--- matrix\n",
    "        df   = pd.DataFrame(np.c_[keys,mat]) #--- data frame\n",
    "        df.sort_values(0,inplace=True)\n",
    "        #    \n",
    "        ngrains_indented = df.shape[0]\n",
    "        ngrains = ngrains_total - ngrains_indented\n",
    "        mat_nan = np.ones(ngrains*n_ld).reshape((ngrains,n_ld))*np.nan\n",
    "        #\n",
    "        keys_nonindented = set(np.arange(ngrains_total))-set(keys)\n",
    "        df_nonindent     = pd.DataFrame(np.c_[list(keys_nonindented),mat_nan])\n",
    "        #--- combine\n",
    "        mat_new = np.concatenate([df,df_nonindent],axis=0)\n",
    "        df      = pd.DataFrame(np.c_[mat_new]).sort_values(0,inplace=False)\n",
    "        return np.c_[df.drop(columns=[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f63e6ea",
   "metadata": {},
   "source": [
    "We use 100 discretization points (`nbins=100`) along the load dimension. The arrays `forces` and `disps` store discretized values per grain. The associated rows corresponding to the non-indented grains are filled with `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7fe4c922",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no GrainIndex for  loadDepth_GrainID_nan_LoadID11_IndentLabel_2.txt\n",
      "no GrainIndex for  loadDepth_GrainID_nan_LoadID11_IndentLabel_3.txt\n",
      "no GrainIndex for  loadDepth_GrainID_nan_LoadID11_IndentLabel_1.txt\n",
      "no GrainIndex for  loadDepth_GrainID_nan_LoadID11_IndentLabel_4.txt\n",
      "# of indented grains:  131\n"
     ]
    }
   ],
   "source": [
    "ngrains = df_attributes.shape[0]\n",
    "nbins   = 100\n",
    "\n",
    "PreprocessLoadData.ld_curve = {}\n",
    "PreprocessLoadData.load     = {}\n",
    "\n",
    "#--- loop over indented grains\n",
    "for key in loadCurves:\n",
    "    data      = loadCurves[ key ]\n",
    "    if np.any(np.isnan(data[:,1])):\n",
    "            print('nan in displacements: ',key)\n",
    "\n",
    "    test_data = PreprocessLoadData( data ) \n",
    "    \n",
    "    #--- interpolation\n",
    "    depth_intrp, load_intrp = test_data.Interp(nbins=nbins) \n",
    "    try:\n",
    "        GrainIndex = test_data.GetGrainIndex(key, df_attributes) #--- could be a nan!\n",
    "    except:\n",
    "        print('no GrainIndex for ',key)\n",
    "        continue\n",
    "        \n",
    "\n",
    "    PreprocessLoadData.Append(GrainIndex,depth_intrp,load_intrp) #--- assemble feature matrix: append displacements\n",
    "    \n",
    "\n",
    "#--- depth data as targets\n",
    "disps      = PreprocessLoadData.BuildFeature(PreprocessLoadData.ld_curve, \n",
    "                                   ngrains,\n",
    "                                   nbins\n",
    "                                  )\n",
    "\n",
    "#---- load data as features\n",
    "forces     = PreprocessLoadData.BuildFeature(PreprocessLoadData.load, \n",
    "                                   ngrains,\n",
    "                                   nbins,\n",
    "                                  )\n",
    "PreprocessLoadData.ReplaceNanByMean(forces)\n",
    "\n",
    "print('# of indented grains: ',(~np.any(np.isnan(disps),axis=1)).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "010fc2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5364, 100)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forces.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52ea9e7",
   "metadata": {},
   "source": [
    "# Build the graph\n",
    "This function loads the nodal and connectivity data and outputs a relevant graph structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a56bf10c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['globals', 'nodes', 'edges', 'receivers', 'senders'])\n",
      "globals\n",
      "nodes\n",
      "edges\n",
      "receivers\n",
      "senders\n",
      "nodes matrix shape: (5364, 112)\n",
      "edge matrix shape: (14302, 2)\n"
     ]
    }
   ],
   "source": [
    "static_graph_tr,\\\n",
    "target_nodes_np, weight_np = base_graph(df_attributes, \n",
    "                                        df_pairwise_attributes,\n",
    "                                        disps,\n",
    "                                        forces,\n",
    "                                        confParser['Parameters']['attributes'].split() #--- attributes\n",
    "                                        )\n",
    "print(static_graph_tr.keys())\n",
    "for k in static_graph_tr.keys():\n",
    "    try:\n",
    "        print(k, static_graph_tr[k].shape)\n",
    "    except AttributeError:\n",
    "        print(k)\n",
    "\n",
    "input_graph  = utils_tf.data_dicts_to_graphs_tuple([static_graph_tr])\n",
    "graphs_tuple = utils_np.data_dicts_to_graphs_tuple([static_graph_tr])\n",
    "\n",
    "print('nodes matrix shape:',np.array(static_graph_tr['nodes']).shape)\n",
    "print('edge matrix shape:',np.array(static_graph_tr['edges']).shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7911df37",
   "metadata": {},
   "source": [
    "# Train-test split\n",
    "Perform a train-test split for the indented grains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f5aa9873",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class train_test_set_split:\n",
    "    '''\n",
    "    Split arrays or matrices into random train and test subsets.\n",
    "    '''\n",
    "    def __init__(self,mask,\n",
    "                 random_state=128,\n",
    "                test_size=.3, train_size=.7):\n",
    "        self.mask         = mask\n",
    "        self.random_state = random_state\n",
    "        self.test_size    = test_size\n",
    "        self.train_size   = train_size\n",
    "        \n",
    "    def train_test_split(self):\n",
    "        '''\n",
    "        random train-test split\n",
    "        '''\n",
    "        n                = self.mask.shape[0]\n",
    "        indented_indices = np.arange(n)[self.mask]\n",
    "        n_indented       = indented_indices.shape[0]\n",
    "        np.random.seed(self.random_state)\n",
    "        np.random.shuffle(indented_indices)\n",
    "        #\n",
    "        m                = int(self.test_size*n_indented)\n",
    "        test_set_indices = indented_indices[:m] #--- test set indices\n",
    "        test_set         = np.zeros(n,dtype=bool)\n",
    "        test_set[test_set_indices]=True #--- assign True to test set indices\n",
    "\n",
    "        #\n",
    "        m                = int(self.train_size*n_indented)\n",
    "        train_set_indices=indented_indices[n_indented-m:n_indented]\n",
    "        train_set        = np.zeros(n,dtype=bool)\n",
    "        train_set[train_set_indices]=True\n",
    "\n",
    "#        return train_set, test_set\n",
    "        self.train_mask = train_set\n",
    "        self.test_mask = test_set\n",
    "    \n",
    "    def train_test_split_structured(self,xy):\n",
    "        '''\n",
    "        structured split\n",
    "        '''\n",
    "        x        =xy[:,0]\n",
    "        y        =xy[:,1]\n",
    "        xlo, xhi = x.min(), x.max()\n",
    "        ylo, yhi = y.min(), y.max()\n",
    "        x_copy   = np.array(x.copy() - xlo)\n",
    "        y_copy   = np.array(y.copy() - ylo)\n",
    "\n",
    "        self.train_mask = np.all([x_copy < self.train_size * (xhi-xlo), self.mask],axis=0)\n",
    "        self.test_mask  = np.all([~self.train_mask, self.mask],axis=0)\n",
    "    \n",
    "    def train_test_split_cv(self,cv=3):\n",
    "        '''\n",
    "        Split arrays or matrices into *random* train and test subsets for cross validation.\n",
    "        '''\n",
    "        assert cv > 1, '# of partitions must be greater than 2'\n",
    "        n                = self.mask.shape[0]\n",
    "        indented_indices = np.arange(n)[self.mask]\n",
    "        n_indented       = indented_indices.shape[0]\n",
    "        np.random.seed(self.random_state)\n",
    "        np.random.shuffle(indented_indices)\n",
    "        m                = int(n_indented/cv)\n",
    "        test_set         = {}\n",
    "        for i in range(cv-1):\n",
    "            test_set[i]  = indented_indices[i*m:(i+1)*m] #--- test set indices\n",
    "        test_set[i+1]    = indented_indices[(i+1)*m:n_indented]\n",
    "        assert n_indented == np.sum(list(map(lambda x:test_set[x].shape[0],test_set.keys())))\n",
    "        #\n",
    "        mask_dic         = {}\n",
    "        for i in range(cv):\n",
    "            tmp_test     = np.zeros(n,dtype=bool)\n",
    "            tmp_test[test_set[i]]=True #--- assign True to test set oindices\n",
    "            tmp_train    = np.all([self.mask,~tmp_test],axis=0)\n",
    "            assert not np.any(np.all([tmp_train,tmp_test],axis=0))\n",
    "            mask_dic[i]  ={}\n",
    "            mask_dic[i]['test'] = np.copy(tmp_test)\n",
    "            mask_dic[i]['train'] = np.copy(tmp_train)\n",
    "\n",
    "        self.mask_dic = mask_dic\n",
    "    \n",
    "    def train_test_split_cv_structured(self,xy,cv):\n",
    "        '''\n",
    "        Split arrays or matrices into *structured* train and test subsets for cross validation.\n",
    "        '''\n",
    "        m        = cv[0]\n",
    "        n        = cv[1]\n",
    "        x        = xy[:,0]\n",
    "        y        = xy[:,1]\n",
    "        xlo, xhi = x.min()-1e-6, x.max()+1e-6\n",
    "        assert xhi > xlo\n",
    "        ylo, yhi = y.min()-1e-6, y.max()+1e-6\n",
    "        assert yhi > ylo\n",
    "        x_copy   = np.array(x.copy() - xlo)\n",
    "        y_copy   = np.array(y.copy() - ylo)\n",
    "        lx       = xhi-xlo\n",
    "        ly       = yhi-ylo\n",
    "        df       = pd.DataFrame(np.c_[list(map(int,m*y_copy / ly)),list(map(int,n*x_copy / lx))],columns=['row','col'])\n",
    "        groups   = df.groupby(by=['row','col']).groups\n",
    "\n",
    "        mask_dic={}\n",
    "        for igroup, count in zip(groups.keys(),range(m*n)):\n",
    "            mask_dic[count] = {}\n",
    "            tmp             = np.zeros(x.shape[0],dtype=bool)\n",
    "            true_indices    = groups[ igroup ]\n",
    "            tmp[true_indices] = True\n",
    "            tmp = np.all([tmp, self.mask],axis=0)\n",
    "            mask_dic[count]['test']  = np.copy(tmp)\n",
    "            mask_dic[count]['train'] = np.all([~tmp, self.mask],axis=0)\n",
    "        #    mask_dic[count]['train'] = np.copy(tmp_train)\n",
    "        self.mask_dic = mask_dic\n",
    "    \n",
    "    def Plot(self,test_data_grains,train_mask,test_mask):\n",
    "        ax=PltErr(test_data_grains[train_mask]['x'],test_data_grains[train_mask]['y'],\n",
    "                  attrs={'fmt':'.'},\n",
    "                 Plot=False\n",
    "                 )\n",
    "\n",
    "        PltErr(test_data_grains[test_mask]['x'],test_data_grains[test_mask]['y'],\n",
    "                      attrs={'fmt':'.','color':'red'},\n",
    "               xstr='x(micron)',ystr='y(micron)',\n",
    "                   ax=ax\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8e5346bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test/train split ...\n",
      "train set (blue) and test set (red):\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('test/train split ...')\n",
    "\n",
    "mask = np.all(~np.isnan(disps),axis=1) #--- indented grains\n",
    "#\n",
    "train_test_mask  = train_test_set_split( mask,\n",
    "                             random_state=eval(confParser['Parameters']['seed']),\n",
    "                             test_size=.3, train_size=.7)\n",
    "\n",
    "#--- train-test split (structured)\n",
    "print('train set (blue) and test set (red):')\n",
    "train_test_mask.train_test_split_structured(np.c_[df_attributes[['x','y']]])\n",
    "train_test_mask.Plot(df_attributes,\n",
    "             train_test_mask.train_mask,\n",
    "             train_test_mask.test_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cc186a",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b560c121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEASURE VALIDATION BY LOSS  True\n",
      "100, training 7.40e+04, test 2.53e+04, best loss inf\n",
      "200, training 7.33e+04, test 2.47e+04, best loss 2.53e+04\n",
      "300, training 7.24e+04, test 2.40e+04, best loss 2.47e+04\n",
      "400, training 7.12e+04, test 2.31e+04, best loss 2.40e+04\n",
      "500, training 6.97e+04, test 2.21e+04, best loss 2.31e+04\n",
      "600, training 6.81e+04, test 2.09e+04, best loss 2.21e+04\n",
      "700, training 6.63e+04, test 1.96e+04, best loss 2.09e+04\n",
      "800, training 6.44e+04, test 1.82e+04, best loss 1.96e+04\n",
      "900, training 6.23e+04, test 1.68e+04, best loss 1.82e+04\n",
      "1000, training 6.02e+04, test 1.53e+04, best loss 1.68e+04\n",
      "1100, training 5.81e+04, test 1.39e+04, best loss 1.53e+04\n",
      "1200, training 5.60e+04, test 1.25e+04, best loss 1.39e+04\n",
      "1300, training 5.39e+04, test 1.11e+04, best loss 1.25e+04\n",
      "1400, training 5.18e+04, test 9.83e+03, best loss 1.11e+04\n",
      "1500, training 4.99e+04, test 8.64e+03, best loss 9.83e+03\n",
      "1600, training 4.80e+04, test 7.56e+03, best loss 8.64e+03\n",
      "1700, training 4.63e+04, test 6.59e+03, best loss 7.56e+03\n",
      "1800, training 4.46e+04, test 5.74e+03, best loss 6.59e+03\n",
      "1900, training 4.31e+04, test 5.01e+03, best loss 5.74e+03\n",
      "2000, training 4.18e+04, test 4.41e+03, best loss 5.01e+03\n",
      "2100, training 4.04e+04, test 3.76e+03, best loss 4.41e+03\n",
      "2200, training 3.92e+04, test 3.25e+03, best loss 3.74e+03\n",
      "2300, training 3.80e+04, test 2.83e+03, best loss 3.24e+03\n",
      "2400, training 3.69e+04, test 2.51e+03, best loss 2.82e+03\n",
      "2500, training 3.59e+04, test 2.26e+03, best loss 2.51e+03\n",
      "2600, training 3.50e+04, test 2.07e+03, best loss 2.26e+03\n",
      "2700, training 3.41e+04, test 1.87e+03, best loss 2.07e+03\n",
      "2800, training 3.33e+04, test 1.69e+03, best loss 1.86e+03\n",
      "2900, training 3.25e+04, test 1.61e+03, best loss 1.69e+03\n",
      "3000, training 3.18e+04, test 1.53e+03, best loss 1.61e+03\n",
      "3100, training 3.11e+04, test 1.39e+03, best loss 1.50e+03\n",
      "3200, training 3.04e+04, test 1.27e+03, best loss 1.37e+03\n",
      "3300, training 2.97e+04, test 1.19e+03, best loss 1.23e+03\n",
      "3400, training 2.91e+04, test 1.19e+03, best loss 1.16e+03\n",
      "3500, training 2.85e+04, test 1.12e+03, best loss 1.15e+03\n",
      "3600, training 2.79e+04, test 1.12e+03, best loss 1.08e+03\n",
      "3700, training 2.73e+04, test 1.19e+03, best loss 1.06e+03\n",
      "3800, training 2.68e+04, test 1.22e+03, best loss 1.06e+03\n",
      "3900, training 2.62e+04, test 1.33e+03, best loss 1.06e+03\n",
      "4000, training 2.57e+04, test 1.41e+03, best loss 1.06e+03\n",
      "4100, training 2.52e+04, test 1.49e+03, best loss 1.06e+03\n",
      "4200, training 2.47e+04, test 1.56e+03, best loss 1.06e+03\n",
      "4300, training 2.42e+04, test 1.60e+03, best loss 1.06e+03\n",
      "4400, training 2.37e+04, test 1.60e+03, best loss 1.06e+03\n",
      "4500, training 2.32e+04, test 1.57e+03, best loss 1.06e+03\n",
      "4600, training 2.26e+04, test 1.50e+03, best loss 1.06e+03\n",
      "4700, training 2.21e+04, test 1.48e+03, best loss 1.06e+03\n",
      "4800, training 2.16e+04, test 1.59e+03, best loss 1.06e+03\n",
      "4900, training 2.11e+04, test 1.62e+03, best loss 1.06e+03\n",
      "5000, training 2.05e+04, test 1.73e+03, best loss 1.06e+03\n",
      "5100, training 2.00e+04, test 1.88e+03, best loss 1.06e+03\n",
      "5200, training 1.95e+04, test 2.10e+03, best loss 1.06e+03\n",
      "5300, training 1.90e+04, test 2.36e+03, best loss 1.06e+03\n",
      "5400, training 1.92e+04, test 7.61e+02, best loss 1.06e+03\n",
      "5500, training 1.84e+04, test 7.50e+02, best loss 6.83e+02\n",
      "5600, training 1.77e+04, test 3.85e+02, best loss 6.83e+02\n",
      "5700, training 1.70e+04, test 4.27e+02, best loss 3.85e+02\n",
      "5800, training 1.64e+04, test 4.34e+02, best loss 3.85e+02\n",
      "5900, training 1.59e+04, test 4.41e+02, best loss 3.85e+02\n",
      "6000, training 1.54e+04, test 4.43e+02, best loss 3.85e+02\n",
      "6100, training 1.50e+04, test 4.42e+02, best loss 3.85e+02\n",
      "6200, training 1.45e+04, test 4.57e+02, best loss 3.85e+02\n",
      "6300, training 1.40e+04, test 4.62e+02, best loss 3.85e+02\n",
      "6400, training 1.36e+04, test 4.72e+02, best loss 3.85e+02\n",
      "6500, training 1.31e+04, test 4.80e+02, best loss 3.85e+02\n",
      "6600, training 1.27e+04, test 4.97e+02, best loss 3.85e+02\n",
      "6700, training 1.23e+04, test 5.18e+02, best loss 3.85e+02\n",
      "6800, training 1.19e+04, test 5.25e+02, best loss 3.85e+02\n",
      "6900, training 1.15e+04, test 5.38e+02, best loss 3.85e+02\n",
      "7000, training 1.11e+04, test 5.50e+02, best loss 3.85e+02\n",
      "7100, training 1.07e+04, test 5.38e+02, best loss 3.85e+02\n",
      "7200, training 1.03e+04, test 5.32e+02, best loss 3.85e+02\n",
      "7300, training 9.97e+03, test 5.49e+02, best loss 3.85e+02\n",
      "7400, training 9.61e+03, test 5.45e+02, best loss 3.85e+02\n",
      "7500, training 9.26e+03, test 5.44e+02, best loss 3.85e+02\n",
      "7600, training 8.91e+03, test 5.40e+02, best loss 3.85e+02\n",
      "7700, training 8.58e+03, test 5.24e+02, best loss 3.85e+02\n",
      "7800, training 8.25e+03, test 5.24e+02, best loss 3.85e+02\n",
      "7900, training 7.92e+03, test 5.05e+02, best loss 3.85e+02\n",
      "8000, training 7.59e+03, test 4.78e+02, best loss 3.85e+02\n",
      "8100, training 7.27e+03, test 4.87e+02, best loss 3.65e+02\n",
      "8200, training 6.95e+03, test 4.71e+02, best loss 3.65e+02\n",
      "8300, training 6.64e+03, test 5.01e+02, best loss 3.48e+02\n",
      "8400, training 6.33e+03, test 4.80e+02, best loss 3.48e+02\n",
      "8500, training 6.03e+03, test 4.89e+02, best loss 3.44e+02\n",
      "8600, training 5.73e+03, test 4.98e+02, best loss 3.42e+02\n",
      "8700, training 5.43e+03, test 4.91e+02, best loss 3.42e+02\n",
      "8800, training 5.15e+03, test 5.28e+02, best loss 3.42e+02\n",
      "8900, training 4.86e+03, test 5.26e+02, best loss 3.42e+02\n",
      "9000, training 4.59e+03, test 5.53e+02, best loss 3.42e+02\n",
      "9100, training 4.32e+03, test 6.38e+02, best loss 3.42e+02\n",
      "9200, training 4.06e+03, test 6.09e+02, best loss 3.42e+02\n",
      "9300, training 3.81e+03, test 6.52e+02, best loss 3.42e+02\n",
      "9400, training 3.57e+03, test 6.86e+02, best loss 3.42e+02\n",
      "9500, training 3.33e+03, test 7.22e+02, best loss 3.42e+02\n",
      "9600, training 3.11e+03, test 7.76e+02, best loss 3.42e+02\n",
      "9700, training 2.89e+03, test 8.82e+02, best loss 3.42e+02\n",
      "9800, training 2.68e+03, test 8.57e+02, best loss 3.42e+02\n",
      "9900, training 2.48e+03, test 9.18e+02, best loss 3.42e+02\n",
      "10000, training 2.29e+03, test 8.99e+02, best loss 3.42e+02\n"
     ]
    }
   ],
   "source": [
    "#--- set training parameters\n",
    "num_processing_steps_tr = 3\n",
    "num_training_iterations = 10000\n",
    "learning_rate           = 1.0e-3 \n",
    "node_output_size        = len(target_nodes_np[0])\n",
    "train_mask              = tf.constant(tsp.train_mask, dtype=tf.bool)\n",
    "test_mask               = tf.constant(tsp.test_mask , dtype=tf.bool)\n",
    "target_nodes            = tf.constant(target_nodes_np)\n",
    "train_mask_np           = train_test_mask.train_mask\n",
    "test_mask_np            = train_test_mask.test_mask\n",
    "\n",
    "\n",
    "model                   = EncodeProcessDecode(node_output_size=len(target_nodes_np[0]))\n",
    "        \n",
    "#--- training base graph\n",
    "output_ops, latent_ops  = model(input_graph, num_processing_steps_tr)\n",
    "\n",
    "#--- Training loss.\n",
    "loss_op_tr              = []\n",
    "loss_op_ts              = []\n",
    "\n",
    "for op in output_ops:\n",
    "    loss_op_tr.append( create_loss_ops(target_nodes, op, train_mask,None))\n",
    "    loss_op_ts.append( create_loss_ops(target_nodes, op, test_mask,None))\n",
    "\n",
    "#--- Training loss across processing steps.\n",
    "loss_op_tr_sum          = sum(loss_op_tr) / num_processing_steps_tr\n",
    "\n",
    "\n",
    "#--- Optimizer\n",
    "optimizer               = tf.train.AdamOptimizer(learning_rate)\n",
    "step_op                 = optimizer.minimize(loss_op_tr_sum)\n",
    "\n",
    "\n",
    "training_history        = np.zeros((num_training_iterations, 2*num_processing_steps_tr)) \n",
    "correlat_history        = np.zeros((num_training_iterations, 2*num_processing_steps_tr)) \n",
    "counter                 = 0\n",
    "\n",
    "\n",
    "#--- This cell resets the Tensorflow session, but keeps the same computational\n",
    "#--- graph.\n",
    "try:\n",
    "  sess.close()\n",
    "except NameError:\n",
    "  pass\n",
    "sess                    = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "#print(snt.format_variables(model.variables))\n",
    "\n",
    "best_val_loss           = np.inf\n",
    "best_val_loss_all       = np.inf*np.ones(num_processing_steps_tr)\n",
    "last_improved           = 0\n",
    "early_stopping_crit     = num_training_iterations\n",
    "\n",
    "measure_val_by_loss     = True\n",
    "print(\"MEASURE VALIDATION BY LOSS \", measure_val_by_loss)\n",
    "\n",
    "# #--- training loop\n",
    "for iteration in range(num_training_iterations):\n",
    "    last_iteration      = iteration\n",
    "    train_values        = sess.run({\n",
    "                                    \"step\": step_op,\n",
    "                                    \"loss\": loss_op_tr,\n",
    "                                    \"outputs\": output_ops,\n",
    "                                    \"latents\": latent_ops,\n",
    "    })\n",
    "\n",
    "\n",
    "    test_values         = sess.run({\n",
    "                                    \"loss_test\": loss_op_ts,\n",
    "    })\n",
    "\n",
    "    #--- store losses at each iteration: training_history[counter]=[train_loss,test_loss]\n",
    "    training_history[counter, 0:num_processing_steps_tr] = train_values['loss']\n",
    "    training_history[counter, num_processing_steps_tr:]  = test_values['loss_test']\n",
    "\n",
    "    #--- print loss corresponding to the last block\n",
    "    if(iteration+1) %100==0:\n",
    "        print(\"%s, training %3.2e, test %3.2e, best loss %3.2e\"\n",
    "              %(iteration+1,training_history[counter,num_processing_steps_tr-1], \n",
    "                training_history[counter,-1], \n",
    "                best_val_loss ))\n",
    "\n",
    "        for i in range(num_processing_steps_tr):\n",
    "            if measure_val_by_loss:\n",
    "                cond      =      (training_history[counter,num_processing_steps_tr+i] < best_val_loss_all[i]) #--- training\n",
    "                cond_best = (training_history[counter,num_processing_steps_tr+i] < best_val_loss)\n",
    "            if cond:\n",
    "                step_output =  sess.run(output_ops[i].nodes) \n",
    "\n",
    "                best_val_loss_all[i]  = training_history[counter,num_processing_steps_tr+i] #--- training\n",
    "            if cond_best:\n",
    "                assert cond\n",
    "                best_output    = np.copy(step_output)\n",
    "                best_latent    = sess.run(latent_ops[i])\n",
    "                best_val_loss  = training_history[counter,num_processing_steps_tr+i] #--- test\n",
    "                last_improved  = counter\n",
    "\n",
    "\n",
    "    counter+=1 \n",
    "    if counter > last_improved + early_stopping_crit:\n",
    "        print('NO IMPROVEMENT IN {} STEPS, STOPPING TRAINING...'.format(int(early_stopping_crit)))\n",
    "        break\n",
    "\n",
    "\n",
    "training_history = training_history[:counter]\n",
    "for i in range(num_processing_steps_tr):\n",
    "    latest_output =  sess.run(output_ops[i].nodes) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "62b12e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_test  = mean_squared_error( np.array(target_nodes_np)[test_mask_np].flatten(),\n",
    "                                np.array(best_output[test_mask_np]).flatten())\n",
    "mse_train = mean_squared_error( np.array(target_nodes_np)[train_mask_np].flatten(),\n",
    "                                np.array(best_output[train_mask_np]).flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec99f217",
   "metadata": {},
   "source": [
    "We'll plot the learning rate associated with the training and test sets here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "402e00d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='epoch', ylabel='mse'>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iproc  = 2\n",
    "legend = Legends()\n",
    "legend.Set()\n",
    "ax     = PltErr(range(training_history.shape[0]),\n",
    "           training_history[:,0*num_processing_steps_tr+iproc],\n",
    "           attrs={'fmt':'x','label':'training'},\n",
    "           xscale='log',\n",
    "           yscale='log',\n",
    "              Plot=False,\n",
    "          )\n",
    "\n",
    "PltErr(range(training_history.shape[0]),\n",
    "           training_history[:,1*num_processing_steps_tr+iproc],\n",
    "           attrs={'fmt':'x','label':'test'},\n",
    "           xscale='log',\n",
    "           yscale='log',\n",
    "           ax=ax,\n",
    "           legend=legend.Get(),\n",
    "           xstr='epoch',\n",
    "           ystr='mse',\n",
    "          )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnnEnv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
